![[image-3.png]]
## scrollToDateTime

- Adds a **one-shot scroll prop** `scrollToDateTime` accepting `Date | date string | number (ms) | null`.
- Scroll triggers **only when the value changes** (time comparison via `getTime()`).
- Ignored when horizontal scroll is externally controlled (`scrollLeft !== null`).
- Invalid date values are safely ignored.
- Target time is **centered when possible**; otherwise it is pinned to the nearest edge (0 or `maxScrollLeft`).
- Requested timestamp is **clamped** within chart bounds (`minimumChartDate` → `maximumChartDate`) before computing scroll.
- Resulting `scrollLeft` is clamped within scrollbar limits (`0..max`) based on `gridWidthPx` and `viewportWidthPx`.
- Uses `effectiveScrollLeft` guard to prevent redundant scroll operations.
- Executes programmatic scroll and triggers `updateScrollLeft`.
- When `scrollWithTimeline` is enabled, viewport continues tracking timeline progression from the new anchor point after jump.
    

---

## scrollWithTimeline

- When `scrollWithTimeline = true`, the chart automatically follows `timelineTime` **only if scroll is uncontrolled** (`scrollLeft === null`).
    Tracking starts from the **current viewport position** rather than re-centering every tick.
    Movement is calculated incrementally using Δ(`timelineTime`) → pixel offset.
    Manual horizontal scrolling while enabled updates the anchor; auto-follow continues from the new position.
    If `scrollToDateTime` is triggered (e.g. using `timelineTime`) and the timestamp is within bounds, viewport jumps to center and then resumes auto-follow if enabled.
- Fixes timeline indicator is **hide** when `timelineTime` is outside the chart range (`chartStart`..`chartEnd`).















==Stopped at Keycloak/Windows authentication==

1. Κριτήριο για το setup σου (RTX 3060 Laptop, 6GB VRAM)

* “Άνετα” σε GPU: 2B–7B μοντέλα, κατά προτίμηση με πιο «ελαφριά» quantization (συνήθως Q4 στα περισσότερα pulls της Ollama).
* “Οριακά”: 8B (μπορεί να κάνει partial offload/πτώση ταχύτητας, ειδικά με μεγάλο context).
* “Μην ασχολείσαι”: 9B+ (σχεδόν πάντα θα σε πάει σε RAM/CPU ή θα είναι ασφυκτικό), και οτιδήποτε 13B+.

2. Φιλτραρισμένη λίστα μοντέλων για να δοκιμάσεις (συνιστώμενα)

* `llama3.2:3b`
* `phi3:mini`
* `gemma2:2b`
* `mistral:7b`
* `qwen2.5:7b` (καλή επιλογή αν το έχεις διαθέσιμο στην Ollama)
* `codellama:7b` (αν σε νοιάζει κυρίως code)

3. Οριακά για 6GB (μόνο αν αποδέχεσαι ότι ίσως δεν είναι full-GPU)

* `llama3.1:8b` ή `llama3:8b` (ανάλογα τι έχεις διαθέσιμο)

  * Αν το τρέξεις, κράτα context χαμηλά (π.χ. 2k–4k) για να μη σε πνίξει το KV cache.

4. Απόφυγε με 6GB

* `gemma2:9b` (συχνά borderline/εκτός)
* `phi3:medium` (14B)
* οτιδήποτε `13b`, `14b`, `27b`, `33b`, `70b`
* multimodal τύπου `llava` (συνήθως θέλουν παραπάνω VRAM για να δουλεύουν άνετα)

5. Πρακτικός έλεγχος ότι όντως πατάει GPU

* Σε άλλο terminal:

```bash
watch -n 1 nvidia-smi
```

* Τρέχεις model, και αν δεις σταθερή VRAM δέσμευση + GPU util να ανεβαίνει, είσαι ΟΚ. Αν VRAM μένει χαμηλά και CPU βαράει, έχεις fallback.

Συμπέρασμα

* Μείνε σε `3b–7b` (llama3.2:3b, phi3:mini, gemma2:2b, mistral:7b, qwen2.5:7b).
* 8B μόνο δοκιμαστικά και με μικρό context. 9B+ άστο.


![[Attachments/Current/2026-02-11/image.png|350]]![[Attachments/Current/2026-02-11/image-1.png|350]]


![[Attachments/Current/2026-02-11/image-4.png|350]]![[Attachments/Current/2026-02-11/image-2.png|350]]

##### Start ollama container
`docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

##### Run model locally
Now you can run a model:

`docker exec -it ollama ollama run llama3`


> [!NOTE] Copied from:
> Docker Hub ollama image info

\>Copied from: Docker Hub ollama image info

