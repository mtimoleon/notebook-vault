Clipped from: [https://mail.google.com/mail/u/0/?zx=tcn2064j5ymd#inbox/FMfcgzGpGdmpGKzbfNkJFbTtdjFMptlh](https://mail.google.com/mail/u/0/?zx=tcn2064j5ymd#inbox/FMfcgzGpGdmpGKzbfNkJFbTtdjFMptlh)

![Exported image](Exported%20image%2020260209130033-0.png)

|   |   |   |   |
|---|---|---|---|
|\|   \|<br>\|---\|<br>\|### The Batch @ DeepLearning.AI [thebatch@deeplearning.ai](mailto:thebatch@deeplearning.ai) [μέσω](https://support.google.com/mail/answer/1311182?hl=el) bf06x.hubspotemail.net\||27 Ιουν 2022, 10:12 μ.μ. (πριν από 11 ώρες)|||
|\|   \|<br>\|---\|<br>\|προς εγώ\|||||

|   |
|---|
|\|   \|<br>\|---\|<br>\|![The Batch_SpecialIssue](Exported%20image%2020260209130035-1.png)\|<br><br>   <br>   <br>   <br>_Dear friends,_  <br>   <br>_Years ago, I had to choose between a neural network and a decision tree learning algorithm. It was necessary to pick an efficient one, because we planned to apply the algorithm to a very large set of users on a limited compute budget. I went with a neural network. I hadn’t used boosted decision trees in a while, and I thought they required more computation than they actually do — so I made a bad call. Fortunately, my team quickly revised my decision, and the project was successful._  <br>   <br>_This experience was a lesson in the importance of learning foundational knowledge and continually refreshing it. If I had refreshed my familiarity with boosted trees, I would have made a better decision._  <br>   <br>_Machine learning, like many technical fields, evolves as the community of researchers builds on top of one another's work. Everything from a housing-price predictor to a text-to-image generator is built on core ideas that include algorithms (linear and logistic regression, decision trees, and so on) and concepts (regularization, optimizing a loss function, bias/variance, and the like)._ <br><br>\|   \|<br>\|---\|<br>\|![ANDREWatWhiteBoardQuestionMARK_1200px](Exported%20image%2020260209130037-2.jpeg)\|<br><br>_Building and maintaining a solid foundation is one key to being a productive machine learning engineer. Many teams draw on these ideas in their day-to-day work, and blog posts and research papers often assume familiarity with them. This shared base of knowledge is essential to the rapid progress we've seen in recent years._  <br>   <br>_That's why I’m updating my original machine learning class as the new_ _Machine Learning Specialization__, which will be available in a few weeks._  <br>   <br>_My team spent many hours debating the most important concepts to teach._ _The result, I hope, is an accessible set of courses that will help anyone master the most important algorithms and concepts in machine learning today — including deep learning but also a lot of other things — and to build effective learning systems._   <br>   <br>_In that spirit, this week’s issue of_ The Batch _explores some of our field’s most important algorithms. If you’re just starting out, I hope it will demystify some of the fundamental approaches in machine learning. For those who are more advanced, I hope you’ll find lesser-known perspectives on familiar territory._   <br>   <br>_Keep learning!_  <br>_Andrew_  <br>   <br>   <br>   <br> <br><br># Essential Algorithms<br><br>Machine learning offers a deep toolbox for solving all kinds of problems, but which tool is best for which task? When is the open-ended wrench better than the adjustable kind? Who invented these things, anyway? In this special issue of _The Batch_, we survey six of the most useful algorithms: where they came from, what they do, and how they’re evolving as AI advances into every facet of society.  <br> <br><br>\|   \|<br>\|---\|<br>\|![LinearRegression_CarWeightMilege_1200px](Exported%20image%2020260209130042-3.jpeg)\|<br><br># Linear Regression: Straight & Narrow <br><br>Linear regression may be the key statistical method in machine learning, but it didn’t get to be that way without a fight. Two eminent mathematicians claimed credit for it, and 200 years later the matter remains unresolved.   <br>Whose algorithm is it anyway? In 1805, French mathematician Adrien-Marie Legendre published the method of fitting a line to a set of points. He was trying to predict the location of a comet. (Celestial navigation was the science most valuable in global commerce at the time, much like AI is today — the new electricity, if you will, two decades before the electric motor.) Four years later, the 24-year-old German wunderkind Carl Friedrich Gauss insisted that he had been using the same method since 1795 but had deemed it too trivial to write about. Gauss’ claim prompted Legendre to publish an addendum anonymously observing that “a very celebrated geometer has not hesitated to appropriate this method.”  <br>Slopes and biases: Linear regression is useful any time the relationship between an outcome and a variable that influences it follows a straight line. For instance, a car’s fuel consumption bears a linear relationship to its weight. <br><br>- The relationship between fuel consumption _y_ and car weight _x_ depends on the line’s slope _w_ (how steeply fuel consumption rises with weight) and bias term _b_ (fuel consumption at zero weight): _y=w*x+b_. <br>- During training, given a car’s weight, the algorithm predicts the expected fuel consumption. It compares expected and actual fuel consumption. Then it minimizes the squared difference, typically via the technique of ordinary least squares, which hones the values of _w_ and _b_.<br>- Taking the car’s drag into account makes it possible to generate more precise predictions. The additional variable extends the line into a plane. In this way, linear regression can accommodate any number of variables/dimensions.<br><br>Two steps to ubiquity: Two further developments unlocked the algorithm’s broad potential. In 1922, English statisticians Ronald Fisher and Karl Pearson showed how linear regression fit into the general statistical framework of correlation and distribution, making it useful throughout all sciences. And, nearly a century later, the advent of computers provided the data and processing power to take far greater advantage of it.  <br>Coping with ambiguity: Of course, data is never perfectly measured, and some variables are more important than others. These facts of life have spurred more sophisticated variants. For instance, linear regression with regularization (also called [ridge regression](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFk3q3npV1-WJV7CgS2kW22H3v0690Q7VW8bmqCT6X-tT9W3YlHh66721BRW3cndrB96PwqFW1HxVJY5B8FH5VwPp6P6Pq2mmW4nWZ-t69W-hXW72cBBB8xpvSTN8vQ7KHG3_p_W59ptC47S7xFvW7Pf8xg7LtX11W2KHDxH3-JmmMW7clB497L7YDCW5Zd0Fy6WkBJjW7jlgxT94K1fJW2Wd7mv7qpKhTW4T0bvt95_pvlN96-fSzbNT9gVF7jzc4WWlRPW4qcFjV1pCMTxW8tvyx56d_vxYF1H581rlHxc38zW1)) encourages a linear regression model not to depend too much on any one variable, or rather to rely evenly on the most important variables. It’s a good default choice. If you’re going for simplicity, a different form of regularization (L1 instead of L2) results in [lasso](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFk3q3npV1-WJV7CgDmKW6PhMMn725bbCVzcHF11S8S3KW20DQ0J30TtSjW31-vBD5bLB9QW9lP--W2DVv5QW7W4xC51vL1mFM-Z7KtwYYPyVZRM5R4QQpDZW9bxYft5xGyfLVP_8qB4Mh7FLW4lt9kn19GYrfW7HxstZ71-m6hW6sHskN7KdX_HN6bGCnkyxBtDW6KVx296rnF1qW5F_30d4vmHYzN1b3TX7J_cFHV9VBht6fg86ZW28pxvk8nly_YN5JmjxdN2gbRW6SnmJq8VmsKHW7Pmf916WXttQ35fV1), which encourages as many coefficients as possible to be zero. In other words, it learns to select variables with high prediction power and ignores the rest. [Elastic net](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFD3q3nJV1-WJV7CgV93N85x7SX9b2TjW8cYgr92VTgQpW7ny_kL5tnVt8W6qdYxF2V9wP5Vx_vKD2Hx0NBVL6D0r4LLQ9wW81RGy_5-hgzwW90VSyp8Y5X-wW4gZ4zV3x23WQW6jvMvw5q-b4ZVljxmY3Rzy1lW72HT9w17bLQDW3tCTjH1rCvPWW8jZCMb57bJbsW275sRK4zp6hYW4-8CHD8SZHx2W8smJs81DZWThMH2lln2H77kW8sSH9T8ZN-BWVpQBZp2PMn6nW10yB1M1yZN3SW5NVTY170SNDtW4XH-CM5jT7L_W68Ll8l2MkfKw3ljy1) combines both types of regularization. It’s useful when data is sparse or features appear to be correlated.  <br>In every neuron: Still, the simple version is enormously useful. The most common sort of neuron in a neural network is a linear regression model followed by a nonlinear activation function, making linear regression a fundamental building block of deep learning.  <br> <br><br>\|   \|<br>\|---\|<br>\|![LogisticRegression_tumbler_1200px](Exported%20image%2020260209130044-4.jpeg)\|<br><br># Logistic Regression: Follow the Curve<br><br>There was a moment when logistic regression was used to classify just one thing: If you drink a vial of poison, are you likely to be labeled “living” or “deceased”? Times have changed. Today, calling emergency services provides a better answer to that question, and logistic regression is at the very heart of deep learning.  <br>Poison control: The logistic function dates to the 1830s, when the Belgian statistician P.F. Verhulst invented it to describe population dynamics: Over time, an initial explosion of exponential growth flattens as it consumes available resources, resulting in the characteristic logistic curve. More than a century passed before American statistician E. B. Wilson and his student Jane Worcester devised logistic regression to figure out how much of a given hazardous substance would be fatal. How they gathered their training data is a subject for another essay.    <br>Fitting the function: Logistic regression fits the logistic function to a dataset in order to predict the probability, given an event (say, ingesting strychnine), that a particular outcome will occur (say, an untimely demise).<br><br>- Training adjusts the curve’s center location horizontally and its middle vertically to minimize error between the function’s output and the data. <br>- Adjusting the center to the right or the left means that it would take more or less poison to kill the average person. A steep slope signifies certainty: Before the halfway point, most people survive; beyond the halfway point, _sayonara_. A gentle slope is more forgiving: lower than midway up the curve, more than half survive. Farther up, less than half.  <br>- Set a threshold of 0.5 between one outcome and another, and the curve becomes a classifier. Just enter the dose into the model, and you’ll know whether you should be planning a party or a funeral.<br><br>More outcomes: Verhulst’s work found the probabilities of binary outcomes, ignoring further possibilities like which side of the afterlife a poison victim might land in. His successors extended the algorithm.<br><br>- Working independently in the late 1960s, British statistician [David Cox](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqGc5nCWtV3Zsc37CgF5_W56Jz-85yF6JRN3g0lcXC6BmvW41zC7f6WFnw3W4s6n4H986ZLZVPMvh66S_GcNV8jm_W3bmFV9MxztBzs0cXsN5VdscH_srVWW2HXbB8743vC1W2ygbLP1YqgYvN7VN3gwtmPY7W1WGWQN3PRHt6VJnhlD5-kBnCW95G_k74RSdyWW1PpyGX8NCtTSW4P_nyk1HzFMVW6csTt-4VPn9SW1w3KQH6rVtXzW2-6W6s6FhQjSW8BwKR97Sg1XQW17tn1q8Vq01lW7Ykf7n3bKTY_W3DYbCH8-3lxhW58nM5v2ygXyhVCDGGs5Jl8ZqW8h-DPT1Qr6S7W5T6DXl6QbyFNW1lTy5K8WRx2NW2DYT918cYpQSVfW2j_35Tx_WN4PzfZsqqjkWW4bYj5L8fyk7KW7hhQ1j5s8HVCW4qzM6490gtMHW1bBF2j5S_D3NW56nNsr2GBx66W7hZn7W8gty3pVmchqD8SFd0CW5pC7665C_gZqN6-KqRrTH8ldW1GJD8c4ZXY3rW1CZgZ_7fZNrPW3J50SV2zj_6lVNW1_-4WkdlX3c-61) and Dutch statistician [Henri Theil](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFD5nCVVV3Zsc37CgBKlW3W1ysK648pjZW2HyLfZ5G56QcW5zGcR96ZWGbZW3KH-wQ1JYFpTW3HgRhQ1qz6kYW8mwfz9905W5dW47RfTB8wKhwfW2TgWnp778VVsW3VVXn96rp2H-W3dS39b2p-W_DW6mjvr43Dn30sW5-h4LW1CKJjsW5qKqq47HYhvJW41m_dp4vCGz-W996WfB4KdlPLV1W74m1YPgzrW24fsLF1f8_vhW1DlH127hRj67W3RDrBg8_bW6vW5DQq4P8ZVFP0N5lnxlv998DqW2gD5cB4m6CHQW23rdZP2mQx7LW4Kmgf46SgvQBW37Jn3w2ztrgwW56WWJn1f1nmvW4N3NV92CLvblW66pmzv47nBsrW44MCCz7PC6KMW531-WM8mD23sN85_w9DstFYLW2DXMT81wzMSYW1Kf_XZ49qYCQW29LPp81sT4pnW3T8-dV6_WXMnW94Q4jn1M5TH3M_ssvlgrzyCW3phF1K6bBlL7W1g_-Pq6l80pGW6dRCtf864NjD3mJ_1) adapted logistic regression for situations that have more than two possible outcomes. <br>- Further work yielded [ordered logistic regression](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqF13q3n5V1-WJV7CgPRMW88BFM854vyr_W3fTfPg3ksrVDW8zlPF13RjrY8W90zVVX89KHwKW7RfjY84FSXPwW1KFw-970CzQrW6cn1lm8pBvl0W2HNRnG6jk-5dN6q-qbBJRvcrW7T2KK01H8fC7W52SG0J30-CPjW5FJs-w1VWVkpVf7xdH6mq4cpW74nBNx6vct-zW51-3Bj23scTLW8MScl73bH4l6W94wJx65Kn8HLVcNMg52R7yF6W38r0-V92M5ySW71Xf4v6PzCFH31fV1), in which the outcomes are ordered values.<br>- To deal with sparse or high-dimensional data, logistic regression can take advantage of the same regularization techniques as linear regression. <br><br>Versatile curve: The logistic function describes a wide range of phenomena with fair accuracy, so logistic regression provides serviceable baseline predictions in many situations. In medicine, it estimates mortality and risk of disease. In political science, it predicts winners and losers of elections. In economics, it forecasts business prospects. More important, it drives a portion of the neurons, in which the nonlinearity is a sigmoid, in a wide variety of neural networks.   <br> <br><br>\|   \|<br>\|---\|<br>\|![HeroesMountainPathsGullies_1200px](Exported%20image%2020260209130045-5.jpeg)\|<br><br># Gradient Descent: It’s All Downhill<br><br>Imagine hiking in the mountains past dusk and finding that you can’t see much beyond your feet. And your phone’s battery died so you can’t use a GPS app to find your way home. You might find the quickest path down via gradient descent. Just be careful not to walk off a cliff.   <br>Suns and rugs: French mathematician Augustin-Louis Cauchy invented the algorithm in 1847 to approximate the orbits of stars. Sixty years later, his compatriot Jacques Hadamard independently developed it to describe deformations of thin, flexible objects like throw rugs that might make a downward hike easier on the knees. In machine learning, though, its most common use is to find the lowest point in the landscape of a learning algorithm’s loss function.  <br>Downward climb: A trained neural network provides a function that, given an input, computes a desired output. One way to train the network is to minimize the loss, or error in its output, by iteratively computing the difference between the actual and desired output and then changing the network’s parameter values to narrow the difference. Gradient descent accomplishes this by minimizing the function that computes the loss.<br><br>- The network’s parameter values are tantamount to a position on the landscape, and the loss is the current altitude. As you descend, you improve the network’s ability to compute outputs close to the desired one. Visibility is limited because, in a typical supervised learning situation, the algorithm relies solely on the network’s parameter values (your position on the hill) and the gradient (the slope immediately beneath your feet).<br>- The basic method is to move in the direction where the terrain descends most steeply. The trick is to calibrate your stride. Too small, and it takes ages to make any progress. Too large, and you leap into the unknown, possibly heading uphill instead of downward.<br>- Given the current position, the algorithm estimates the direction of steepest descent by computing the gradient of the loss function. The gradient points uphill, so the algorithm steps in the opposite direction by subtracting a fraction of the gradient. The fraction _α_, which is called the learning rate, determines the size of the step before measuring the gradient again.<br>- Apply this iteratively, and hopefully you’ll arrive at a valley.  <br><br>Stuck in the valley: Too bad your phone is out of juice, because the algorithm may not have propelled you to the bottom of a convex mountain. Instead, you may be stuck in a nonconvex landscape of multiple valleys (local minima), peaks (local maxima), saddles (saddle points), and plateaus. In fact, tasks like image recognition, text generation, and speech recognition are nonconvex, and many variations on gradient descent have emerged to handle such situations. For example, the algorithm may have [momentum](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqF13q3n5V1-WJV7CgQ6CW4X3mYk2XSnD8N6KV0r4ZyM-VW8HtB_G7p4dLsW6gh-w051Lw86W6JJjMT5Z1pkXW85vRMC7xCh-BW1r88C38R5-tdW32c-cd2X_4MlVrRbmq6YZmgwN26JpyPrZxTHW4sZjWj55JLNpW2DvLlh5K9190W2rVYy329lD0jVN5ZFL4gkgTpW4jDmkx6x9vX4V8xPB13JJR_lVDDKbk7hkWfpW6CFPDg8BhdtPW1Vq47V450vZ7W5LwHbV6G0cLH33jK1) that helps it zoom over small rises and dips, giving it a better chance at arriving at the bottom. Luckily, local and global minima tend to be [roughly equivalent](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqF13q3n5V1-WJV7CgJkMW49LzCC7Rw3v1W6f4tGl1-dPqSW7Ywwlb3S5m_pW5TqDPZ8YyNfxW1sN6cQ7QtVpNVJhFk03JLCqhN7mQKB41Wph5W2VBrkb3j7M0CVX6XnL5L9SPDVWszDm1tGFlGN39ZSZPGL0Z9W7_RcwY14N4KGW1ZPkKm3qFvMZW1Mw3hd3wXrzpVcjLNY2WM15TW2dDpMm1nZwfJW8VYZY45BTPpnW8YhcW572P0lqW6XwZjj5l90XSW94_Rxl2tJ0wy3fJr1).  <br>Optimal optimizer: Gradient descent is the clear choice for finding the minimum of any function. In cases where an exact solution can be computed directly — say, a linear regression task with lots of variables — it can approximate one, often faster and more cheaply. But it really comes into its own in complex, nonlinear tasks. Armed with gradient descent and an adventurous spirit, you might just make it out of the mountains in time for dinner.  <br> <br><br>\|   \|<br>\|---\|<br>\|![AdmiralPerceptron_1200px](Exported%20image%2020260209130047-6.jpeg)\|<br><br># Neural Networks: Find the Function<br><br>Let’s get this out of the way: A brain is not a cluster of graphics processing units, and if it were, it would run software far more complex than the typical artificial neural network. Yet neural networks were inspired by the brain’s architecture: layers of interconnected neurons, each of which computes its own output depending on the states of its neighbors. The resulting cascade of activity forms an idea — or recognizes a picture of a cat.  <br>From biological to artificial: The insight that the brain learns through interactions among neurons dates back to 1873, but it wasn’t until 1943 that American neuroscientists Warren McCulloch and Walter Pitts modeled biological neural networks using simple mathematical rules. In 1958, American psychologist Frank Rosenblatt developed the [perceptron](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFX3q3n_V1-WJV7CgW4SW2kxcff1hX-Z7W1R03K-5kpD8TW9kjvhk6SCzR-W2blTZ46wQ24pW3xNL8l7RzRY2W3jXJkn1n-JmpW4bWtNH97jSN9W2466qg7hRfvbVjGnC42pN_VzW3cN6Q-3pjZ8kW36zsHG5lvkr_W7dfg6p7P_MJnW7PKSN15GVH3xW58CC-L402XcbW9m22t-2HvY4rW6kPkD03Df4qnV7ZgBW7-SxhjW4y0__V8nPGbLW47QN1V9fNd27W7pl8nJ1GnjD7Mf0C9zxgQrtW8M3nnL64RLjmW1__MDf5NDMBlN8kZ0md7S_sCW7zGW8S1X4clTW5R3cjq8_h3R938-T1), a single-layer vision network implemented on punch cards with the intention of building a hardware version for the United States Navy.  <br>Bigger is better: Rosenblatt’s invention recognized only classes that could be separated by a line. Ukrainian mathematicians Alexey Ivakhnenko and Valentin Lapa overcame this limitation by [stacking](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFX3q3n_V1-WJV7CgQ8KW783dNt9c3ygXW7FlDMT9kjbKqW6jfSP85f7277W41-xyf1RxC1WW6YLcQ14ZP_CNW83KfLY26Rxs4N3qXyvFgPph_W4GyP3G7bd57DW33kfTS5MNQP8W4XZlZN38ldLLN238KZP_p7DkW6lmngQ42F8RDVG8qGl1DZZKXW2c1lD_1TSwG5N8cGbGpqfJLHW2rFsSz49Sh6rVt7yf06-JVL3N7N9GGbtw7b2N33C4jyjfmQNW7d1xR95DL-wZW6KkwX56CQLKhW7F_3pW8s5PkbVkDWq63rnyd9W1kBmbM7pvRHYV_nXvT5WrSvwW7hW4696drBtz32wG1) networks of neurons in any number of layers. In 1985 and 1986, working independently, French computer scientist Yann LeCun, David Parker, and American psychologist David Rumelhart and his colleagues [described](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFk3q3npV1-WJV7CgMmpW7T9w_895nR-LW4RFnF38HzNfSW425Rpz8jm9jSN6x0sy9Pc4MkN62vJTdnl0ScVW28W94lzfzBW8qXbst3dhK80N4VgsxB3SDdsW3V-s3g711tyLW2f86-S6j9rNpW8f8CND4M_lPCW7Y4bBM7QdsRWN1qCRwJqGq9PW8MkYrb3B1bQWW3JQLQh72JJQcN80knDksjj7cN60pNjGMlVxjW5XsGKs2HK-9SW8F8DSK9gGvLlW8Y5JxQ1tdCRNW3NGTB_65QTJhN8m4q6DFwvHF2BD1) using backpropagation to train neural networks efficiently, following earlier work by Finnish mathematician Seppo Linnainmaa and American social scientist Paul Werbos. In the 2000s, researchers including Kumar Chellapilla, Dave Steinkraus, and Rajat Raina (with Andrew Ng) [accelerated](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFk3q3npV1-WJV7CgW32N8pcK_bZh8lpVlbrM867h-RJW5pM9Xf1nBYTtN7yzpDT8hBGwW3QsvK56ZmylgW2B01Wj7TYMmnW25C97q1lb1tbW54Z4FK7yR72QN5HpbKyTL5WsVYTg3W878T3hVhdLN461R4PjW2mR14l6g0H1hW3NkgMG1rYwk0Vp0DkZ6MtxJwW4xWxTB50YJYkN83tC5PtrnlGW4QP3kw1SFz5fW12S6X07bwLyVN8wn9yr4Sgg2W7HT5Wl5KDMmHW24Nk5c6c_N3_W8NV_lH6HgDBt32xJ1) neural networks using chips designed to accelerate computer graphics, or graphical processing units. This development has enabled ever-larger neural networks to learn from the immense amounts of data generated by the internet.  <br>Fit for every task: The idea behind a neural network is simple: For any task, there’s a function that can perform it. A neural network constitutes a trainable function by combining many simple functions, each executed by a single neuron. A neuron’s function is determined by adjustable parameters (also called weights). Given random parameter values and examples of inputs and their desired outputs, it’s possible to alter those values iteratively until the trainable function performs the task at hand.<br><br>- A neuron accepts various inputs (for example, numbers representing a pixel or word, or the outputs of the previous layer), multiplies them by its parameter values, adds the products, and feeds the sum through a nonlinear function, or activation function, chosen by the developer. Consider it linear regression plus an activation function. <br>- Training modifies the parameter values. For every example input, the network computes an output and compares it to the expected output. Backpropagation uses gradient descent to change the values to reduce the difference between actual and expected outputs. Repeat this process enough times with enough (good) examples, and the network should learn to perform the task.<br><br>Toward common sense: Reporting on Rosenblatt’s Perceptron in 1958, The New York Times called it “the embryo of an electronic computer that the United States Navy expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.” While it didn’t live up to that billing, it begot models have exceeded human-level performance in playing Go and approached it in diagnosing x-ray images. Yet neural networks still have a hard time with common sense and logical reasoning. Ask GPT-3, “When counting, what number comes before a million?” and it may [reply](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFD3q3nJV1-WJV7CgRn8W58y6B3136Pl9W5cZwKN4D23bFW2HsQ8f3sZzN-W5KZlCH9dTV-xN4bNfVkz4t-dW5-FW_N7Y34WrW6JDqXT3zmvMhW7GRBLK4RwQCmW32K0SR90-9nGW5sw0Sf6j_7gTW8-qGxc2B5l__W3glLrY6R1c-YW41vFX025jmVlW4lWYHW6rMdhsW5-zxyk3N3hGZW1rP-BY3FhfHsW2Sp39-78m4hkW7KcHzz3FjVTtW7WV_7x6vsMlqW8dSfRN8VKqdsW5k9gT-6xP09TTxxG335ms0BW1Zvvjy61pDyYW5xdMkQ8GJ7RR3pBC1), “Nine hundred thousand and ninety-nine comes before a million.” To which we reply: Keep learning!  <br> <br><br>\|   \|<br>\|---\|<br>\|![DecisionTree_1200px](Exported%20image%2020260209130050-7.jpeg)\|<br><br># Decision Trees: From Root to Leaves<br><br>What kind of beast was Aristotle? The philosopher's follower Porphyry, who lived in Syria during the third century, came up with a logical way to answer the question. He organized Aristotle’s proposed “categories of being” from general to specific and assigned Aristotle himself to each category in turn: Aristotle’s substance occupied space rather than being conceptual or spiritual; his body was animate not inanimate; his mind was rational not irrational. Thus his classification was human. Medieval teachers of logic drew the sequence as a vertical flowchart: An early decision tree.  <br>The digital difference: Fast forward to 1963, when University of Michigan sociologist John Sonquist and economist James Morgan, dividing survey respondents into groups, first [implemented](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqDL5nCT_V3Zsc37CgXhZW4Kqgz93VDPkNW5C8Psb6hqb1dW98SnL17C8HgsW8TqYnJ1y_p-_W4ZwV5R1lLqwpW3XW9t47GtbrgW3Yzwrw68TXLmW8XDp282fKBt2W7v8KlJ10_CtKW31zSsw3-T-b-W8gSYn-4rGd5CW1qgmWs2pW7qpW4T04107p7p4BW2MfxlB1XydPmW4P2DSv7yFYxCV-_BNL93fW0sW3_SChn1PvGnjW9lgtLl1YbGTPW6rmmWd4ZXD99W5vbxFL4JjL4DW8km6Z818jGvjVtC6g62tTR5SVQDcbN7YZmXBW5JzbsN6F5fR0W24Bqd944TBtKW9kNR272McgB6W1gw6CW33WP_yW5hQ49R1_GVcnW17d9_W88Gp-6W3cfXhD67c3GkW2639bp2PZlgnW3-DFBy333KSWVGBbFq5crTxGW4qcSkm3XzBTx39R_1) decision trees in a computer. Such work became commonplace with the advent of software that automates training the algorithm, now available in a variety of machine learning libraries including scikit-learn. The code took a quartet of statisticians at Stanford and UC Berkeley 10 years to develop. Today, coding a decision tree from scratch is a homework assignment in Machine Learning 101.  <br>Roots in the sky: A decision tree can perform classification or regression. It grows downward, from root to canopy, in a hierarchy of decisions that sort input examples into two (or more) groups. Consider the task of Johann Blumenbach, the German physician and anthropologist who first distinguished monkeys from apes (setting aside humans) circa 1776, before which they had been categorized together. The classification depends on various criteria such as presence or absence of a tail, narrow or broad chest, upright versus crouched posture, and lesser or greater intelligence. A decision tree trained to label such animals would consider each criterion one by one, ultimately separating the two groups.<br><br>- The tree starts with a root node that can be viewed as containing all examples in a dataset of creatures — chimpanzees, gorillas, and orangutans as well as capuchins, baboons, and marmosets. The root presents a choice between examples that exhibit a particular feature or not, leading to two child nodes that contain examples with and without that feature. Each child poses yet another choice that leads to two more children, and so on. The process ends with any number of leaf nodes, each of which, mostly or wholly, contains examples of one class.<br>- To grow, the tree must find the root decision. To choose, it considers all features and their values — posterior appendage, barrel chest, and so on — and chooses the one that maximizes the purity of the split. (Optimal purity is defined as 100 percent of examples of one class going to a particular child node and none going to the other node.) Splits are rarely 100 percent pure after just one decision and may never get there, so the process continues, producing level after level of child nodes, until purity doesn’t rise much by considering further features. At this point, the tree is fully trained.<br>- At inference, a fresh example traverses the tree, which evaluates a different decision at each level from top to bottom. The example takes the label of the data contained by the leaf node it lands in.<br><br>Top 10 hit: In 1986, Australian computer scientist John Ross Quinlan extended decision trees to support nonbinary outcomes with [ID3](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFk3q3npV1-WJV7CgTdGN10HRfXBc1WsW5Jv2hc5fWQ8dV_sr646tQQSKN5LNh732VQJcW5tfqlK4mVcR6Vstncb8JR1YpW6sML6h88KssYW2f-ck_20CGfQW4n5PP24QDkbGW4ZmCvD5BJYRkW3_HRYM52CY4LW6VQHZD4HqwJ1N1xLZLMFmSr3W6Fz3DT858FgtW2yQF1m6cXlLPW2cctzs2PxXYyVyxZt41Jb9mZW2BcYhw9kn5lpW25_HRL9dCdKDW4WZlSB4CzH3HW80LpqT8gg4PqW4JK0H-2VYd_m33sJ1). In 2008, a further refinement called [C4.5](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFk3q3npV1-WJV7CgQ7YW4s3Ng01NJVXBW95V1P51qj_30VyZmpK16qmLPW857vHM1KsW_jW49C6Vv56-SSCW6FLkDp7hTYVXW41P4lv4mNBmyW5sJctC98ld0KW5Z-zdS3V-1Q3W5zyHcx72_64JW2w1Tbz2DX5vnW475Hv33phfnJW1zgsCY7rh0HzW6MXrKk37fnpQW8W7-Bq6G8h6SW6KLJ4G5n5mcPW3wcRnZ1G8W4yW4NR5bz2Rs9r7W3411v_4c3cGcW2wphrP7dQHmlW862bLp1VDLBmW2z_G2w8yc-Br36hF1) capped a list of Top 10 Algorithms in Data Mining curated by the IEEE International Conference on Data Mining.   <br>Into the woods: Decision trees do have some drawbacks. They can easily overfit the data by growing so many levels that leaf nodes include as few as one example. Worse, they’re prone to the butterfly effect: Change one example, and the tree that grows could look dramatically different. Turning this trait into an advantage, American statistician Leo Breiman and New Zealander statistician Adele Cutler in 2001 developed the [random forest](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFD3q3nJV1-WJV7CgRlDW2nFC-W24kL1FW6gKp8D2B6gJHN7vXpMNpyMjBN2HPZ_L9mQMcW4m_qLd4rW9J4V-359f4-qYm9W7Tfjkt3n58fZVsypyn3dlR_2N4h6fcZstzX0VW6Yqv89yVnLW6Q8Tzk7KvRgvW2QLw011WnGW1W2YlCx016B84tVzhvtd9gD4_4W5kVyjD17FgYsVK03Dm5NB7r6W1Tw0866b7rc0VWzXlZ8dwqrvN3N6-JC2V5QXW1QFdJk8WFtQ9W27F0RM7qpXPHW4Njdk21tlXwVW16fxrd2MRTHrW5M403x7NqNwl3fFZ1), an ensemble of decision trees, each of which processes a different, overlapping selection of examples that vote on a final decision. Random forest and its cousin XGBoost are less prone to overfitting, which helps make them among the most popular machine learning algorithms. It’s like having Aristotle, Porphyry, Blumenbach, Darwin, Jane Goodall, Dian Fossey, and 1,000 other zoologists in the room together, all making sure your classifications are the best they can be.  <br> <br><br>\|   \|<br>\|---\|<br>\|![KMeans_3Clusters_1200px_Crop](Exported%20image%2020260209130051-8.jpeg)\|<br><br># K-Means Clustering: Group Think<br><br>If you’re standing close to others at a party, it’s likely you have something in common. This is the idea behind using k-means clustering to split data points into groups. Whether the groups formed via human agency or some other force, this algorithm will find them.   <br>From detonations to dial tones: American physicist Stuart Lloyd, an alumnus of both Bell Labs’ iconic innovation factory and the Manhattan Project that invented the atomic bomb, first proposed k-means clustering in 1957 to distribute information within digital signals. He didn’t publish it until 1982. Meanwhile, American statistician Edward Forgy described a similar method in 1965, leading to its alternative name, the Lloyd-Forgy algorithm.   <br>Finding the center: Consider breaking up the party into like-minded working groups. Given the positions of attendees in the room and the number of groups to be formed, k-means clustering can divide the attendees into a given number of groups of roughly equal size.<br><br>- During training, the algorithm initially designates k cluster center points, or centroids, by randomly choosing k people. (K must be chosen manually, and finding an optimal value is not always trivial.) Then it grows k clusters by associating each person to the closest centroid.<br>- For each cluster, it computes the mean position of all people assigned to the group and designates the mean position as the new centroid. The new centroids may not be occupied by a person, but so what? People tend to gather around the chocolate fondue.<br>- Having calculated new centroids, the algorithm reassigns individuals to the centroid closest to them. Then it computes new centroids, adjusts clusters, and so on, until the centroids (and the groups around them) no longer shift.<br>- From there, assigning newcomers to the right cluster is easy. Let them take their place in the room and look for the nearest centroid.<br><br>Different distances: The distance between clustered objects doesn’t need to be spatial. Any measure between two vectors will do. For instance, rather than grouping partygoers according to physical proximity, k-means clustering can divide them by their outfits, occupations, or other attributes. Online shops use it to partition customers based on their preferences or behavior, and astronomers to group stars of the same type.  <br>Power to the data points: The idea has spawned a few notable variations:<br><br>- [K-medoids](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFD3q3nJV1-WJV7CgLSlW8wv1MS14Hbr0W2RyL7C1zJlcjW967VLK2NM1qzVjrZwG8ftR3yN60JCMDPDs3NW5RGl1n1B1n7jW4C_Zmz2rG-_pV1yQyk201v2pW13cmC023_ZbbW5d3jhm6qVJyMW81GHgj8N2sFyW5b5N-c9k1rPZW5gt6hm9hjV2jW2G-FFy4WW6F3W97jHW25XpppZVFhyb962K2yvW8MzgLP8xt1tSW6by61_4cY8sWW7bCZXr2ngLTMW4x8M5h62JwyxV7j5Qq7H52xcN95TQWMxgb-mVYLTD-26133-W2lYg9t1QB8kN3cFm1) use actual data points as centroids rather than mean positions in a given cluster. The medoids are points that minimize the distance to all other points in their cluster. This variation is more interpretable because the centroids are always data points.<br>- [Fuzzy C-Means Clustering](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFD3q3nJV1-WJV7CgQ55N4bfkDfdvStzW8wgGSD184m3_W7dNJcs2kMpvPW34cQNb4JkT3rW60LPRs26hrWQW7rTkS-8td__cW98BLL41TNlY_W5GB1sd4H58DbW3X9qxM725xnxW4nrkc24qY5bFW8H33J16PqtJ6N71V4wtDb4vFW30dK4y3BvZmCW8JCqTV2JM78sW2D-2RN4tbn78W7K_D143cNMxRW3rv5Gb6bHF3pW7GY1Sz1MfwcPW5y2gvN5wmmBQVBm-7t5_1Z7_W5W8Bnn1JDXZnW7gFRWw6bQdnnVbTplf2D6Dy5W6t50MK6FFdTq3c8x1) enables the data points to participate in multiple clusters to varying degrees. It replaces hard cluster assignments with degrees of membership depending on distance from the centroids.<br><br>Revelry in n dimensions: Nonetheless, the algorithm in its original form remains widely useful — especially because, as an unsupervised algorithm, it doesn’t require gathering potentially expensive labeled data. It’s also ever faster to use. For instance, machine learning libraries including scikit-learn benefit from the 2002 addition of [kd-trees](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVNFv37RkCfjW1dbHHG2Gr28CW178YCN4LQQ8pN3ypqFk3q3npV1-WJV7CgYpNW2243qN6KQHhDW7-xNm-6WLH8cW7zh10C23kJBqW8kqx1D1XTKnzW2cnvd_1gq3N6W6VgKL-2sxN53W5H7Fkm7c0SRkW2V98pZ82p9sTW6kwCYF5QWVMTVB-jDB1BV0mXN3GMCRBxx4fYW92Tl9D5y0w4tW21Ytr318pYy3VNcHQp6qs4n7W8y787h5B8rGSW47j5zy8SxQRrW64kXm471hXtFW7BZjX27vJSk6N8JZtVRJRR3rVBqygs52dpQyW3jG6z86Rk_WzVL2__H7JlQVc3bZp1) that partition high-dimensional data extremely quickly. By the way, if you throw any high-dimensional parties, we’d love to be on the guest list.|